{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import zscore\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from numpy.random import randn\n",
    "from scipy.stats import pearsonr\n",
    "from numpy import corrcoef\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from matplotlib import pyplot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a dataset to test Linear regression and ridge regression\n",
    "\n",
    "##### Add some noise to dataset that can be resolved with regularization such as multicollinearity, lesser number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset(n_samples, n_features, effective_rank, noise, random_state=6, coef=True):\n",
    "    # generate regression dataset\n",
    "    X, y, coef = make_regression(n_samples=n_samples, \n",
    "                                 n_features=n_features,  \n",
    "                                 effective_rank=effective_rank,\n",
    "                                 noise=noise,\n",
    "                                 random_state=random_state,\n",
    "                                 coef=coef)   \n",
    "    # add later:\n",
    "    # bias, n_informative\n",
    "    X = pd.DataFrame(X)\n",
    "    y = pd.Series(y)\n",
    "    X.columns = [f'col_{i+1}' for i in range(len(X.columns))]\n",
    "    \n",
    "    return X, y, coef\n",
    "\n",
    "\n",
    "# noise --- The standard deviation of the gaussian noise applied to the output.\n",
    "# effective_rank --- The approximate number of singular vectors required to explain most of the input data by linear combinations. \n",
    "# Using this kind of singular spectrum in the input allows the generator to reproduce the correlations often observed in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coef_data = generate_dataset(n_samples=5000, n_features=10, effective_rank=1, noise=0.3)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_norm = scaler.fit_transform(X)\n",
    "X_norm = pd.DataFrame(X_norm)\n",
    "X_norm.columns = [f'col_{i+1}' for i in range(len(X.columns))]\n",
    "# X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Scikit-learn algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.98, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Linear Regression -----\n",
      "Cross validation scores : [0.9625622  0.95329889 0.90517589 0.9482027  0.92770664]\n",
      "Test score : 0.9521110550872984\n"
     ]
    }
   ],
   "source": [
    "linear = LinearRegression()\n",
    "\n",
    "print(\"----- Linear Regression -----\")\n",
    "val_scores = cross_val_score(linear, X_train, y_train, cv=5)\n",
    "print(f'Cross validation scores : {val_scores}')\n",
    "\n",
    "linear.fit(X_train, y_train)\n",
    "test_score = linear.score(X_test, y_test)\n",
    "print(f'Test score : {test_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- Ridge Regression -----\n",
      "Cross validation scores : [0.96567773 0.94949791 0.90408329 0.95208436 0.92852214]\n",
      "Test score : 0.953313396380002\n"
     ]
    }
   ],
   "source": [
    "ridge = Ridge(alpha=1)\n",
    "\n",
    "print(\"----- Ridge Regression -----\")\n",
    "val_scores = cross_val_score(ridge, X_train, y_train, cv=5)\n",
    "print(f'Cross validation scores : {val_scores}')\n",
    "\n",
    "ridge.fit(X_train, y_train)\n",
    "test_score = ridge.score(X_test, y_test)\n",
    "print(f'Test score : {test_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on custom algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataset\n",
    "X, y, coef_data = generate_dataset(n_samples=5000, n_features=10, effective_rank=1, noise=0.3)\n",
    "\n",
    "# normalize features\n",
    "X_norm = pd.DataFrame()\n",
    "for col in X.columns:\n",
    "    X_norm[col] = normalize(X[col])\n",
    "    \n",
    "# split dataset into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.98, random_state=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(col):\n",
    "    '''\n",
    "    normalize each variable between 0 to 1\n",
    "    with mean zero and std. deviation equal to 1 \n",
    "    '''\n",
    "    mean = col.mean()\n",
    "    std_dev = col.std()\n",
    "    col = [(x - mean)/std_dev for x in col]\n",
    "    \n",
    "    return col\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    score = r2_score(y_true, y_pred)\n",
    "    return score\n",
    "\n",
    "def predict(feat, coef):\n",
    "    '''\n",
    "    Make prediction for a row (instance)\n",
    "    '''\n",
    "    y_pred = coef[0]\n",
    "    \n",
    "    for i in range(feat.shape[0]):\n",
    "        y_pred += coef[i+1] * feat[i]\n",
    "    return y_pred\n",
    "\n",
    "\n",
    "def sgd_linear(train_X, label, lr, n_epoch):\n",
    "  \n",
    "    coef = [0.0 for i in range(train_X.shape[1] + 1)]\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for lbl, (idx, x) in zip(label, train_X.iterrows()):\n",
    "            y_pred = predict(x, coef)\n",
    "            loss = y_pred - lbl\n",
    "            total_loss += loss**2\n",
    "            coef[0] = coef[0] - lr * loss    # update bias coefficient\n",
    "            \n",
    "            for i in range(train_X.shape[1]):\n",
    "                coef[i+1] = coef[i+1] - lr * loss * x[i]    # update features' coefficients\n",
    "                \n",
    "#         print('>epoch=%d, learning_rate=%.3f, total_loss=%.3f' % (epoch, lr, total_loss))\n",
    "    return coef\n",
    "\n",
    "\n",
    "\n",
    "def sgd_ridge(train_X, label, lr, n_epoch, alpha):\n",
    "  \n",
    "    coef = [0.0 for i in range(train_X.shape[1] + 1)]\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        total_loss = 0\n",
    "        \n",
    "        for lbl, (idx, x) in zip(label, train_X.iterrows()):\n",
    "            y_pred = predict(x, coef)\n",
    "            loss = y_pred - lbl\n",
    "            total_loss += loss**2\n",
    "            coef[0] = coef[0] - lr * loss    # update bias coefficient\n",
    "            \n",
    "            for i in range(train_X.shape[1]):\n",
    "                coef[i+1] = coef[i+1] - lr * ((loss * x[i]) + (alpha * coef[i+1]))    # update features' coefficients\n",
    "             \n",
    "    return coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_derivative_bias(label, train_X, coef):\n",
    "    \n",
    "    derivative = 0\n",
    "    for lbl, (idx, x) in zip(label, train_X.iterrows()):\n",
    "        y_pred = predict(x, coef)\n",
    "        loss = y_pred - lbl\n",
    "        der = loss\n",
    "        derivative += der\n",
    "        \n",
    "    return derivative/train_X.shape[0]\n",
    "\n",
    "\n",
    "def get_derivative_coef(label, train_X, coef, j):\n",
    "    \n",
    "    derivative = 0\n",
    "    for lbl, (idx, x) in zip(label, train_X.iterrows()):\n",
    "        y_pred = predict(x, coef)\n",
    "        loss = y_pred - lbl\n",
    "        der = loss * x[j]\n",
    "        derivative += der\n",
    "        \n",
    "    return derivative/train_X.shape[0]\n",
    "\n",
    "def get_derivative_coef_bgd(label, train_X, coef, alpha, j):\n",
    "    \n",
    "    derivative = 0\n",
    "    for lbl, (idx, x) in zip(label, train_X.iterrows()):\n",
    "        y_pred = predict(x, coef)\n",
    "        loss = y_pred - lbl\n",
    "        der = loss * x[j]\n",
    "        derivative += der\n",
    "        \n",
    "    derivative += alpha * coef[j+1]\n",
    "    return derivative/train_X.shape[0]\n",
    "\n",
    "\n",
    "def bgd_linear(train_X, label, lr, n_epoch):\n",
    "  \n",
    "    coef = [0.0 for i in range(train_X.shape[1] + 1)]\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        # get bias derivative, then update bias\n",
    "        derivative_bias = get_derivative_bias(label, train_X, coef)\n",
    "        coef[0] = coef[0] - lr * derivative_bias\n",
    "        \n",
    "        for j in range(train_X.shape[1]):\n",
    "            \n",
    "            # get derivative w.r.t j-th coef, then update coef\n",
    "            derivative_coef = get_derivative_coef(label, train_X, coef, j)\n",
    "            coef[j+1] = coef[j+1] - lr * derivative_coef\n",
    "                        \n",
    "    return coef\n",
    "\n",
    "\n",
    "def bgd_ridge(train_X, label, lr, n_epoch, alpha):\n",
    "  \n",
    "    coef = [0.0 for i in range(train_X.shape[1] + 1)]\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        \n",
    "        # get bias derivative, then update bias\n",
    "        derivative_bias = get_derivative_bias(label, train_X, coef)\n",
    "        coef[0] = coef[0] - lr * derivative_bias\n",
    "        \n",
    "        for j in range(train_X.shape[1]):\n",
    "            \n",
    "            # get derivative w.r.t j-th coef, then update coef\n",
    "            derivative_coef = get_derivative_coef_bgd(label, train_X, coef, alpha, j)\n",
    "            coef[j+1] = coef[j+1] - lr * derivative_coef\n",
    "                        \n",
    "    return coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data\n",
    "# coef_sgd_linear = sgd_linear(X_train, y_train, 0.01, 10)\n",
    "# coef_sgd_ridge = sgd_ridge(X_train, y_train, 0.01, 10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coef_bgd_linear = bgd_linear(X_train, y_train, 0.01, 400)    # try increasing learning rate\n",
    "coef_bgd_ridge = bgd_ridge(X_train, y_train, 0.02, 400, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BGD Score : 0.9432390741231034\n"
     ]
    }
   ],
   "source": [
    "# Evaluate linear reg - SGD\n",
    "# y_preds_linear_sgd = []\n",
    "# for _, feat in X_test.iterrows():\n",
    "#     y_pred = predict(feat, coef_sgd_linear)\n",
    "#     y_preds_linear_sgd.append(y_pred)\n",
    "    \n",
    "# sgd_score = score(y_test, y_preds_linear_sgd)\n",
    "# print(f'SGD Score : {sgd_score}')\n",
    "\n",
    "\n",
    "# Evaluate linear reg - BGD\n",
    "y_preds_linear_bgd = []\n",
    "for _, feat in X_test.iterrows():\n",
    "    y_pred = predict(feat, coef_bgd_linear)\n",
    "    y_preds_linear_bgd.append(y_pred)\n",
    "    \n",
    "bgd_score = score(y_test, y_preds_linear_bgd)\n",
    "print(f'BGD Score : {bgd_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9538302014993534"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Train ridge regression\n",
    "# coef_ridge = coef_sgd_ridge(X_train, y_train, 0.001, 400, 1)\n",
    "\n",
    "# # Evaluate ridge reg\n",
    "# y_preds_ridge = []\n",
    "# for _, feat in X_test.iterrows():\n",
    "#     y_pred = predict(feat, coef_ridge)\n",
    "#     y_preds_ridge.append(y_pred)\n",
    "    \n",
    "# score(y_test, y_preds_ridge)\n",
    "\n",
    "\n",
    "# Evaluate ridge reg\n",
    "y_preds_ridge_bgd = []\n",
    "for _, feat in X_test.iterrows():\n",
    "    y_pred = predict(feat, coef_bgd_ridge)\n",
    "    y_preds_ridge_bgd.append(y_pred)\n",
    "    \n",
    "score(y_test, y_preds_ridge_bgd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning points\n",
    "\n",
    "1. Wrote stochastic gradient descent algorithm from scratch\n",
    "2. Exploding loss values (There was no error when tested with scikit-learn libraries)\n",
    "3. Why batch GD might be more effective than SGD?\n",
    "  (SGD fluctuates too much before converging, not enough data)\n",
    "4. You can mention performance difference SGD vs BGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyplot.scatter(X[:, 2], y)\n",
    "# pyplot.show()\n",
    "\n",
    "# from scipy.linalg import eigh, cholesky\n",
    "# from scipy.stats import norm\n",
    "\n",
    "# num_samples = 100\n",
    "# r = np.array([\n",
    "#         [  3.40, -2.75, -2.00],\n",
    "#         [ -2.75,  5.50,  1.50],\n",
    "#         [ -2.00,  1.50,  1.25]\n",
    "#     ])\n",
    "\n",
    "# c = cholesky(r, lower=True)\n",
    "# x = norm.rvs(size=(3, num_samples))\n",
    "# y = np.dot(c, x)\n",
    "# y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
